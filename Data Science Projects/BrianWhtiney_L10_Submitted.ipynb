{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In this assignment, we want to implement cross-validation for logistic regression. Cross-validation is a powerful technique for model selection (such as when choosing the right hyper-parameters), especially when the data size is not very large. The goal of this assignment is to first implement cross-validation and compare it to a baseline model (with no cross-validation).\n",
    "\n",
    "1. Refactor the code from the lab and train and evaluate the `LogisticRegression` classifier just like we did in the lab. <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "boston = load_boston()\n",
    "df_boston = pd.DataFrame(boston['data'], columns = boston['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates training and test data from Boston dataset\n",
    "df_boston['is_above_40k'] = boston['target'] >= 40\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_boston.drop(columns = 'is_above_40k'), \n",
    "                                                    df_boston['is_above_40k'], \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Runs a basic Logistic regression\n",
    "logit = LogisticRegression(max_iter=5000)\n",
    "\n",
    "logit.fit(x_train, y_train)\n",
    "y_test_pred = logit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98        95\n",
      "        True       1.00      0.43      0.60         7\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.98      0.71      0.79       102\n",
      "weighted avg       0.96      0.96      0.95       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints stats\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The `LogisticRegression` classifier has an argument called `class_weight`. Read the documentation to see what it does, then train a new model this time by providing the class weights. <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs a Logistic regression with balanced class weights to account for uneven catagories\n",
    "logit_balanced = LogisticRegression(max_iter = 5000, class_weight = 'balanced')\n",
    "logit_balanced.fit(x_train, y_train)\n",
    "y_test_pred_balanced = logit_balanced.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Does it change any of the results? In what way? <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.97      0.97        95\n",
      "        True       0.57      0.57      0.57         7\n",
      "\n",
      "    accuracy                           0.94       102\n",
      "   macro avg       0.77      0.77      0.77       102\n",
      "weighted avg       0.94      0.94      0.94       102\n",
      "\n",
      "Basic:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98        95\n",
      "        True       1.00      0.43      0.60         7\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.98      0.71      0.79       102\n",
      "weighted avg       0.96      0.96      0.95       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints stats\n",
    "print('Balanced:')\n",
    "print(classification_report(y_test, y_test_pred_balanced))\n",
    "print('Basic:')\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This follows the problem discussed in class where the \"Highest Accuracy\" model is not necessarily the best model for future data given the weight of the sample data. for example 96% of the sample data are of one category and 4% are not. It is of higher accuracy given our test data to just always assume it is of the original category regardless of the traits of the second category. By adding weights we decrease our accuracy (94 vs 96) but add the classification of a second option rather than our earlier generalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Return to the training step but use `LogisticRegressionCV` this time (the CV stands for cross-validation). <span style=\"color:red\" float:right>[1 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Runs a Logistic regression Cross validaiton with 5 folds and 10,000 max iterations\n",
    "logitCV = LogisticRegressionCV(cv=5, max_iter = 10000)\n",
    "logitCV.fit(x_train, y_train)\n",
    "y_test_pred_CV = logitCV.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Does cross-validation seem to make a difference in the results we get? <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98        95\n",
      "        True       1.00      0.43      0.60         7\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.98      0.71      0.79       102\n",
      "weighted avg       0.96      0.96      0.95       102\n",
      "\n",
      "Cross Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98        95\n",
      "        True       1.00      0.43      0.60         7\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.98      0.71      0.79       102\n",
      "weighted avg       0.96      0.96      0.95       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints stats\n",
    "print('Basic:')\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print('Cross Validation:')\n",
    "print(classification_report(y_test, y_test_pred_CV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO. The cross validation does not appear to make a diffrence. At least not in this rounded format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Change the number of folds from 5 to 10 and train the CV model again? Notice any difference in performance? Note that *performance* here refers to the model's overall accuracy, based on your choice of metric, it does NOT refer to run-time. <span style=\"color:red\" float:right>[3 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs a Logistic regression Cross validaiton with 10 folds and 10,000 max iterations\n",
    "logitCV10 = LogisticRegressionCV(cv=10, max_iter = 10000)\n",
    "logitCV10.fit(x_train, y_train)\n",
    "y_test_pred_CV10 = logitCV10.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98        95\n",
      "        True       1.00      0.43      0.60         7\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.98      0.71      0.79       102\n",
      "weighted avg       0.96      0.96      0.95       102\n",
      "\n",
      "Cross Validation 10:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      1.00      0.98        95\n",
      "        True       1.00      0.43      0.60         7\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.98      0.71      0.79       102\n",
      "weighted avg       0.96      0.96      0.95       102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Cross Validation 5:')\n",
    "print(classification_report(y_test, y_test_pred_CV))\n",
    "print('Cross Validation 10:')\n",
    "print(classification_report(y_test, y_test_pred_CV10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What was the cost of increasing the number of folds in terms of run-time? <span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.48 s, sys: 47.2 ms, total: 9.52 s\n",
      "Wall time: 9.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Sets a timer for the cell\n",
    "\n",
    "# same as above\n",
    "logitCV = LogisticRegressionCV(cv=5, max_iter = 10000)\n",
    "logitCV.fit(x_train, y_train)\n",
    "y_test_pred_CV = logitCV.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.5 s, sys: 98.5 ms, total: 16.6 s\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Sets a timer for the cell\n",
    "\n",
    "# same as above\n",
    "logitCV10 = LogisticRegressionCV(cv=10, max_iter = 10000)\n",
    "logitCV10.fit(x_train, y_train)\n",
    "y_test_pred_CV10 = logitCV10.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going from 5 to 10 folds takes approx. 2 times as long\n"
     ]
    }
   ],
   "source": [
    "print(\"going from 5 to 10 folds takes approx.\", round(16.7/9.54),\"times as long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
